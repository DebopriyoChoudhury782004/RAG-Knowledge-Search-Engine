# 🧠 RAG Knowledge Search Engine

A **Retrieval-Augmented Generation (RAG)** based **Knowledge Search Engine** that allows users to upload multiple documents (PDFs or text files), perform semantic retrieval using embeddings, and generate **LLM-powered synthesized answers** to user queries.

---

## 🚀 Project Overview

The **Knowledge Search Engine** combines **document retrieval** and **large language model (LLM)** capabilities to answer user queries using information extracted directly from uploaded knowledge sources.

### 🎯 Objective

> To enable users to search across multiple documents and obtain **accurate, synthesized answers** through Retrieval-Augmented Generation (RAG).

---

## 🧩 Features

✅ **Document Ingestion** – Load and process multiple text/PDF files  
✅ **Vector Embeddings** – Convert text into numerical vectors for semantic search  
✅ **Similarity Search** – Retrieve the most relevant document chunks  
✅ **LLM-based Answer Generation** – Generate human-like, context-aware responses  
✅ **REST API (FastAPI)** – Backend for document upload, query handling, and response generation  
✅ **Optional Streamlit UI** – Simple frontend for query input and answer display

---

## 🏗️ System Architecture

```
Documents → Embedding Generation → Vector Store (FAISS)
           ↓
      User Query → Similarity Search → LLM → Synthesized Answer
```

---

## ⚙️ Tech Stack

| Component                  | Technology                                |
| -------------------------- | ----------------------------------------- |
| **Backend**                | FastAPI                                   |
| **Language Model (LLM)**   | GPT-based model (OpenAI API or similar)   |
| **Embeddings**             | Sentence Transformers / OpenAI Embeddings |
| **Vector Store**           | FAISS                                     |
| **Frontend (Optional)**    | Streamlit                                 |
| **File Formats Supported** | `.pdf`, `.txt`, `.md`                     |

---

## 📂 Project Structure

```
RAG-Knowledge-Search-Engine/
├──.env                   # Environment variables
├── app.py                # FastAPI backend entry point
├── ingest.py             # Document ingestion and embedding creation
├── qa_chain.py           # RAG pipeline and LLM query synthesis
├── streamlit_app.py      # Optional Streamlit frontend
├── requirements.txt      # Project dependencies
├── README.md             # Project documentation
└── docs/                 # Folder for source documents
└── faiss_index/          # vector database
```

---

## 🧠 Core Workflow

1. **Document Ingestion**

   - Load and chunk documents using `ingest.py`.
   - Generate embeddings for each chunk and store them in FAISS.

2. **Query Handling**

   - User enters a question.
   - Retrieve top-matching document chunks using similarity search.

3. **Answer Generation**
   - Pass retrieved context + question to an LLM (via `qa_chain.py`).
   - Return a synthesized, context-aware response.

---

## 🔧 Setup Instructions

### 1️⃣ Clone the Repository

```bash
git clone https://github.com/DebopriyoChoudhury782004/RAG-Knowledge-Search-Engine.git
cd RAG-Knowledge-Search-Engine
```

### 2️⃣ Create and Activate Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # On Windows use: venv\Scripts\activate
```

### 3️⃣ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4️⃣ Set Up Environment Variables

Create a `.env` file in the project root and add your OpenAI API key:

```
OPENAI_API_KEY=your_openai_api_key
```

### 5️⃣ Run the Backend Server

```bash
uvicorn app:app --reload
```

The API will be available at:  
👉 `http://127.0.0.1:8000`

### 6️⃣ (Optional) Run Streamlit Frontend

```bash
streamlit run streamlit_app.py
```

---

## 🧪 Example Usage

1. Upload documents using the ingestion endpoint or script.
2. Enter a query such as:
   > “Summarize the key points about data retrieval in these documents.”
3. Get a synthesized, fact-based answer generated by the LLM.

---

## 📊 Evaluation Focus

| Metric                 | Description                                  |
| ---------------------- | -------------------------------------------- |
| **Retrieval Accuracy** | How relevant the retrieved chunks are        |
| **Answer Quality**     | How well the LLM synthesizes context         |
| **Code Structure**     | Modularity and readability of implementation |
| **LLM Integration**    | Efficiency and correctness of RAG pipeline   |

---

## 🧾 Deliverables

- ✅ Full GitHub repository with working backend
- ✅ Well-documented README
- ✅ Demo video showcasing ingestion and QA workflow

---

## 💡 Future Improvements

- Add support for **multiple embedding models**
- Implement **hybrid search (BM25 + embeddings)**
- Add **user authentication** for personalized document sets
- Deploy on **Render / Hugging Face Spaces / Streamlit Cloud**

---

## 👨‍💻 Author

**Debopriyo Choudhury**  
📧 [debopriyochoudhury782004@gmail.com](mailto:debopriyochoudhury782004@gmail.com)  
🔗 [GitHub Profile](https://github.com/DebopriyoChoudhury782004)

---

## 🪪 License

This project is licensed under the **MIT License** – feel free to use, modify, and distribute.

---

### ⭐ If you found this project helpful, consider giving it a star on GitHub!
