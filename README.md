# ğŸ§  RAG Knowledge Search Engine

A **Retrieval-Augmented Generation (RAG)** based **Knowledge Search Engine** that allows users to upload multiple documents (PDFs or text files), perform semantic retrieval using embeddings, and generate **LLM-powered synthesized answers** to user queries.

---

## ğŸš€ Project Overview

The **Knowledge Search Engine** combines **document retrieval** and **large language model (LLM)** capabilities to answer user queries using information extracted directly from uploaded knowledge sources.

### ğŸ¯ Objective

> To enable users to search across multiple documents and obtain **accurate, synthesized answers** through Retrieval-Augmented Generation (RAG).

---

## ğŸ§© Features

âœ… **Document Ingestion** â€“ Load and process multiple text/PDF files  
âœ… **Vector Embeddings** â€“ Convert text into numerical vectors for semantic search  
âœ… **Similarity Search** â€“ Retrieve the most relevant document chunks  
âœ… **LLM-based Answer Generation** â€“ Generate human-like, context-aware responses  
âœ… **REST API (FastAPI)** â€“ Backend for document upload, query handling, and response generation  
âœ… **Optional Streamlit UI** â€“ Simple frontend for query input and answer display

---

## ğŸ—ï¸ System Architecture

```
Documents â†’ Embedding Generation â†’ Vector Store (FAISS)
           â†“
      User Query â†’ Similarity Search â†’ LLM â†’ Synthesized Answer
```

---

## âš™ï¸ Tech Stack

| Component                  | Technology                                |
| -------------------------- | ----------------------------------------- |
| **Backend**                | FastAPI                                   |
| **Language Model (LLM)**   | GPT-based model (OpenAI API or similar)   |
| **Embeddings**             | Sentence Transformers / OpenAI Embeddings |
| **Vector Store**           | FAISS                                     |
| **Frontend (Optional)**    | Streamlit                                 |
| **File Formats Supported** | `.pdf`, `.txt`, `.md`                     |

---

## ğŸ“‚ Project Structure

```
RAG-Knowledge-Search-Engine/
â”œâ”€â”€.env                   # Environment variables
â”œâ”€â”€ app.py                # FastAPI backend entry point
â”œâ”€â”€ ingest.py             # Document ingestion and embedding creation
â”œâ”€â”€ qa_chain.py           # RAG pipeline and LLM query synthesis
â”œâ”€â”€ streamlit_app.py      # Optional Streamlit frontend
â”œâ”€â”€ requirements.txt      # Project dependencies
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ docs/                 # Folder for source documents
â””â”€â”€ faiss_index/          # vector database
```

---

## ğŸ§  Core Workflow

1. **Document Ingestion**

   - Load and chunk documents using `ingest.py`.
   - Generate embeddings for each chunk and store them in FAISS.

2. **Query Handling**

   - User enters a question.
   - Retrieve top-matching document chunks using similarity search.

3. **Answer Generation**
   - Pass retrieved context + question to an LLM (via `qa_chain.py`).
   - Return a synthesized, context-aware response.

---

## ğŸ”§ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/DebopriyoChoudhury782004/RAG-Knowledge-Search-Engine.git
cd RAG-Knowledge-Search-Engine
```

### 2ï¸âƒ£ Create and Activate Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # On Windows use: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Environment Variables

Create a `.env` file in the project root and add your OpenAI API key:

```
OPENAI_API_KEY=your_openai_api_key
```

### 5ï¸âƒ£ Run the Backend Server

```bash
uvicorn app:app --reload
```

The API will be available at:  
ğŸ‘‰ `http://127.0.0.1:8000`

### 6ï¸âƒ£ (Optional) Run Streamlit Frontend

```bash
streamlit run streamlit_app.py
```

---

## ğŸ§ª Example Usage

1. Upload documents using the ingestion endpoint or script.
2. Enter a query such as:
   > â€œSummarize the key points about data retrieval in these documents.â€
3. Get a synthesized, fact-based answer generated by the LLM.

---

## ğŸ“Š Evaluation Focus

| Metric                 | Description                                  |
| ---------------------- | -------------------------------------------- |
| **Retrieval Accuracy** | How relevant the retrieved chunks are        |
| **Answer Quality**     | How well the LLM synthesizes context         |
| **Code Structure**     | Modularity and readability of implementation |
| **LLM Integration**    | Efficiency and correctness of RAG pipeline   |

---

## ğŸ§¾ Deliverables

- âœ… Full GitHub repository with working backend
- âœ… Well-documented README
- âœ… Demo video showcasing ingestion and QA workflow

---

## ğŸ’¡ Future Improvements

- Add support for **multiple embedding models**
- Implement **hybrid search (BM25 + embeddings)**
- Add **user authentication** for personalized document sets
- Deploy on **Render / Hugging Face Spaces / Streamlit Cloud**

---

## ğŸ‘¨â€ğŸ’» Author

**Debopriyo Choudhury**  
ğŸ“§ [debopriyochoudhury782004@gmail.com](mailto:debopriyochoudhury782004@gmail.com)  
ğŸ”— [GitHub Profile](https://github.com/DebopriyoChoudhury782004)

---

## ğŸªª License

This project is licensed under the **MIT License** â€“ feel free to use, modify, and distribute.

---

### â­ If you found this project helpful, consider giving it a star on GitHub!
